# 实验03 流分类

#### 介绍

网络测量实验三：流分类



#### 数据集下载

~~~
https://pan.seu.edu.cn:443/link/342494DF0BC8A78BC67A964754C8D99A
有效期限：2028-05-15
访问密码：kBt5
MD5 Hash:  E61304CD25296E18C200A84CA9893A17
~~~



#### 信息增益

信息增益（Information Gain）是决策树算法中的一个重要概念，主要用于选择特征，用以区分训练数据中的不同类别。在ID3决策树算法中，信息增益越大的特征越有区分能力，因此，我们需要选择具有最大信息增益的特征作为当前节点的划分特征。

在了解信息增益之前，需要先了解熵（Entropy）的概念。同学们应该在信息论这门课中学过，熵是表示数据集不确定性的度量。一个数据集的熵越大，其信息越混乱。熵的计算公式如下：

$$
H(D) = -\sum_{i=1}^{n} p_i \log_2{p_i}
$$

其中，$H(D)$ 表示数据集 $D$ 的熵，$p_i$ 是数据集中第 $i$ 个类别的概率。

信息增益是基于熵的概念计算的。信息增益表示在特征 $A$ 的条件下，数据集 $D$ 的熵的减少量。信息增益越大，表示特征 $A$ 对数据集 $D$ 的分类能力越强。信息增益公式如下：

$$
G(D, A) = H(D) - H(D|A)
$$

其中，$G(D, A)$ 表示特征 $A$ 对数据集 $D$ 的信息增益，$H(D)$ 是数据集 $D$ 的熵，$H(D|A)$ 是在特征 $A$ 的条件下数据集 $D$ 的条件熵。条件熵计算公式如下：

$$
H(D|A) = \sum_{j=1}^{m} \frac{|D_j|}{|D|} H(D_j)
$$

其中，$m$ 是特征 $A$ 的取值个数，$|D_j|$ 是特征 $A$ 取值为第 $j$ 个值时，数据集 $D$ 中的样本数，$|D|$ 是数据集 $D$ 的总样本数，$H(D_j)$ 是特征 $A$ 取值为第 $j$ 个值时，数据集 $D$ 的熵。



下面通过一个选西瓜🍉的例子说明信息增益的计算：

假设有以下数据集，包含5个样本：

| 色泽 | 根蒂 | 是否为好瓜 |
| :--- | :--- | :--------- |
| 青绿 | 蜷缩 | 是         |
| 乌黑 | 蜷缩 | 是         |
| 乌黑 | 稍蜷 | 是         |
| 青绿 | 硬挺 | 否         |
| 浅白 | 硬挺 | 否         |

目标是预测是否为好瓜，我们需要计算每个特征的信息增益，以选择最佳的划分特征。

首先，计算数据集的熵：

$$
H(D) = -\frac{3}{5} \log_2{\frac{3}{5}} - \frac{2}{5} \log_2{\frac{2}{5}} \approx 0.971
$$

接下来，计算特征 "色泽" 的条件熵：

$$
H(D|\text{色泽}) = \frac{2}{5}H(D_{\text{青绿}}) + \frac{2}{5}H(D_{\text{乌黑}}) + \frac{1}{5}H(D_{\text{浅白}}) = \frac{2}{5}(-\frac{1}{2}\log_2{\frac{1}{2}} - \frac{1}{2}\log_2{\frac{1}{2}}) + \frac{2}{5}(-\frac{2}{2}\log_2{\frac{2}{2}}) + 0 = 0.4
$$

计算特征 "色泽" 的信息增益：

$$
G(D, \text{色泽}) = H(D) - H(D|\text{色泽}) = 0.971 - 0.4 \approx 0.571
$$

同理，计算特征 "根蒂" 的条件熵和信息增益：

计算特征 "根蒂" 的条件熵：

$$
H(D|\text{根蒂}) = \frac{2}{5}H(D_{\text{蜷缩}}) + \frac{2}{5}H(D_{\text{稍蜷}}) + \frac{1}{5}H(D_{\text{硬挺}}) = \frac{2}{5}(-\frac{2}{2}\log_2{\frac{2}{2}}) +\frac{2}{5}(-\frac{1}{2}\log_2{\frac{1}{2}} - \frac{1}{2}\log_2{\frac{1}{2}}) + 0 = 0.4
$$

计算特征 "根蒂" 的信息增益：

$$
G(D, \text{根蒂}) = H(D) - H(D|\text{根蒂}) = 0.971 - 0.4 \approx 0.571
$$

根据以上计算结果，特征 "色泽" 的信息增益为 0.571，特征 "根蒂" 的信息增益为 0.571。在本例中，信息增益相等，说明这两个特征对数据集的分类能力一样强。在实际操作中，我们可以根据具体需求选择一个特征进行划分。


#### 各类统计特征含义

1. `mean` : 均值，表示数据集中所有数值的平均值。
2. `median` : 中位数，表示数据集排序后位于中间位置的数值。
3. `mode` : 众数，表示数据集中出现次数最多的数值。
4. `variation_ratio` : 变异率，表示不等于众数的数据个数与总数据个数之比。
5. `quartile_deviation` : 四分位数差，表示数据集的上四分位数（Q3）与下四分位数（Q1）之差。
6. `range` : 极差，表示数据集的最大值与最小值之差。
7. `mean_deviation` : 平均绝对偏差，表示数据集中每个数值与均值的绝对差的平均值。
8. `variance` : 方差，表示数据集中每个数值与均值之差的平方的平均值。
9. `coefficient of variation` : 变异系数，表示数据集的标准差与均值之比。
10. `SK` : 偏度（Skewness），表示数据集的分布形态的偏斜程度。偏度为正表示数据右偏（有长尾），为负表示数据左偏（有长尾）。
11. `K` : 峰度（Kurtosis），表示数据集的分布形态的峰态。峰度大于 0 表示分布形态较陡峭，小于 0 表示分布形态较平坦。


#### 具体任务

1. 处理提供的流量数据集：下载原始的pcap数据包，在实验二的基础上将每个pcap文件组流，补全`def extractFlow(flow_list)` 函数，提取五元组及其对应的报文长度序列，并按类别输出到csv文件；
2. 计算信息增益：补全ID3决策树算法中的`def cal_best_theta_value(self, ke, attri_list)`函数，该函数通过最大化信息增益来计算给定特征的最佳阈值；
3. 流量分类：利用设计的ID3决策树算法结合11个流统计特征对流量进行分类，得到分类准确率，理解根据数据包长度序列统计特征进行流分类的完整流程；
4. 实验优化：可考虑通过修改特征、优化模型等策略提高分类准确率。（选做）

完成实验后的项目文件格式应如下：

~~~
.
├── chat-label.csv
├── chat.csv
├── data
│  ├── chat
│  │  ├── WeChat1.exe.pcap
│  │  ├── WeChat2.exe.pcap
│  │  ├── WeChat3.exe.pcap
│  │  ├── WeChat4.exe.pcap
│  │  ├── WeChat5.exe.pcap
│  │  ├── WeChat6.exe.pcap
│  │  ├── WeChat7.exe.pcap
│  │  ├── WeChat8.exe.pcap
│  │  ├── WeChat9.exe.pcap
│  │  └── WeChat10.exe.pcap
│  ├── video
│  │  ├── YoukuNplayer1.exe.pcap
│  │  ├── YoukuNplayer2.exe.pcap
│  │  ├── YoukuNplayer3.exe.pcap
│  │  ├── YoukuNplayer4.exe.pcap
│  │  └── YoukuNplayer5.exe.pcap
│  └── web
│     ├── msedge1.exe.pcap
│     ├── msedge2.exe.pcap
│     ├── msedge3.exe.pcap
│     ├── msedge4.exe.pcap
│     └── msedge5.exe.pcap
├── data.zip
├── flow_combine.py
├── label.py
├── MultiClassDTree.py
├── net_steams_classification.py
├── README.md
├── requirements.txt
├── video-label.csv
├── video.csv
├── web-label.csv
└── web.csv
~~~



#### 提交细则

1. 5月30日前以“网络测量课程实验三-学号-姓名”的形式将代码和实验报告上传至Gitee，严禁抄袭！若错过上交时间或代码报告雷同，本次实验成绩为0，希望大家认真独立思考，及时上交。
2. 提交文件包括：实验报告(pdf格式，使用报告模板，附带实验结果图)、实验中需要补全的代码、实验过程中得到的csv文件。



#### 参考资料

1. [ID3 algorithm - Wikipedia](https://en.wikipedia.org/wiki/ID3_algorithm)
2. [Decision tree learning - Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)